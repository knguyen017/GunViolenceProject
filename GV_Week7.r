read.csv("/Users/KristineNguyen/Documents/tweets.csv", stringsAsFactors = FALSE)
m <- leaflet(mymap) %>% addTiles()
read.csv("/Users/KristineNguyen/Documents/tweets.csv", stringsAsFactors = FALSE)
gunViolence <- read.csv("/Users/KristineNguyen/Documents/tweets.csv", stringsAsFactors = FALSE)
plot(table(gunViolence$created), type = "b", main="frequence of daily gun violence tweets" ,xlab="day",ylab="number of tweets",col="blue")
)
gv
View(gv)
GunViolence <- read.csv("~/Documents/GunViolence.csv", comment.char="#")
gv <- read.delim("~/Documents/gv.txt", comment.char="#")
tweets.text <- tolower(gv)
t<- tolower(gv)
gv_new <- Corpus(VectorSource(gv))
install.packages("tm")
library(tm)
gv_new <- Corpus(VectorSource(gv))
inspect(gv_new)
glimpse(gv_new)
gv_new <- VCorpus(gv)
gv <- read.delim("~/Documents/gv.csv", comment.char="#", stringsAsFactors=FALSE)
tweets.text <- tolower(gv)
tweets.text <- gsub("rt", "", tweets.text)
tweets.text <- gsub("@\\w+", "", tweets.text)
tweets.text <- gsub("[[:punct:]]", "", tweets.text)
tweets.text <- gsub("http\\w+", "", tweets.text)
gv <- read.delim("~/Documents/GunViolence_NoDups.csv", comment.char="#", stringsAsFactors=FALSE)
tweets.text <- tolower(gv)
tweets.text <- gsub("rt", "", tweets.text)
tweets.text <- gsub("@\\w+", "", tweets.text)
tweets.text <- gsub("[[:punct:]]", "", tweets.text)
tweets.text <- gsub("^ ", "", tweets.text)
tweets.text <- gsub(" $", "", tweets.text)
tweets.text.corpus <- Corpus(VectorSource(tweets.text))
tweets.text.corpus <- tm_map(tweets.text.corpus, function(x)removeWords(x,stopwords()))
wordcloud(tweets.text.corpus,min.freq = 2, scale=c(7,0.5),colors=brewer.pal(8, "Dark2"),  random.color= TRUE, random.order = FALSE, max.words = 150)
library(wordcloud)
wordcloud(tweets.text.corpus,min.freq = 2, scale=c(7,0.5),colors=brewer.pal(8, "Dark2"),  random.color= TRUE, random.order = FALSE, max.words = 150)
install.packages("SentimentAnalysis")
library(SentimentAnalysis)
sentiment <- analyzeSentiment("We can't make people think how we think if there will is strong. Money is a strong motivation they gotta keep they_„_ https://t.co/VFkrBQatRN")
convertToBinaryResponse(sentiment)$SentimentQDAP
sentiment <- analyzeSentiment(gv)
convertToBinaryResponse(sentiment)$SentimentQDAP
library(SnowballC)
View(GunViolence)
tweets_df = twListToDF(GunViolence)
library(twitteR)
tweets_df = twListToDF(GunViolence)
View(gv)
View(sentiment)
mycorpus = Corpus(VectorSource(gv$text))
mycorpus$content
rURL <- function(x) gsub("http[^[:space:]]*", "", x)
mycorpus <- tm_map(mycorpus, content_transformer(rURL))
rnp <- function(x) gsub("[^[:alpha:][:space:]]*", "", x)
mycorpus <- tm_map(mycorpus,content_transformer(rnp))
mycorpus <- tm_map(mycorpus, removeWords, stopWords("english"))
mycorpus <- tm_map(mycorpus, removeWords, stopWords('english'))
response <- convertToDirection(sentiment$SentimentQDAP)
compareToResponse(sentiment, response)
head(response)
library(dplyr)
library(plyr)
ldply(response)
x <- ldply(response)
resp <- list(x)
resp
View(x)
View(sentiment)
View(x)
savehistory("~/Desktop/week6gv.r")
compareToResponse(sentiment, x)
convertToBinaryResponse(response)
compareToResponse(sentiment, resp)
df <- data.frame(No=1:5, Sentiment=sentiment)
df <- data.frame(No=1:5, Sentiment=sentiment$SentimentQDAP)
response2 <- convertToBinaryResponse(sentiment$SentimentQDAP)
head(response2)
head(response)
require(devtools)
as.data.frame(response)
compareToResponse(sentiment, resp)
compareToResponse(sentiment, response)
as.data.frame(response2)
compareToResponse(sentiment, response)
compareToResponse(sentiment, response2)
plotSentimentResponse(sentiment$SentimentQDAP, response)
plotSentimentResponse(sentiment$SentimentQDAP, response2)
View(sentiment)
write.csv(sentiment,"sa.csv")
responseSA <- read.csv("~/Desktop/sa.csv", sep="")
View(responseSA)
plotSentimentResponse(sentiment$SentimentQDAP, responseSA)
responseSA <- read.csv("~/Desktop/sa.csv", sep="")
View(responseSA)
compareToResponse(sentiment, responseSA)
sa <- as.numeric(responseSA)
as.data.frame(responseSA)
sa <- as.numeric(responseSA)
sa <- as.numeric(unlist(responseSA)
e
sa <- as.numeric(unlist(responseSA))
compareToResponse(sentiment, sa)
plotSentimentResponse(sentiment$SentimentQDAP, sa)
savehistory("~/part2.r")
install.packages("RSiteCatalyst")
install.packages("RTextTools")
findFreqTerms(tweet.text, lowfreq=20)
View(gv)
gv_kmeans <- create_matrix(gv$text, stemwords=TRUE, removeStopwords= FALSE,
minWordLength=1, removePunctuation=TRUE)
install.packages(c('tm', 'SnowballC', 'wordcloud', 'topicmodels'))
install.packages(c("tm", "SnowballC", "wordcloud", "topicmodels"))
library(tm)
library(SnowballC)
library(wordcloud)
gv_corpus = Corpus(VectorSource(gv$text))
gv_corpus = tm_map(review_corpus, content_transformer(tolower))
gv_corpus = tm_map(gv_corpus, content_transformer(tolower))
gv_corpus =tm_map(gv_corpus, removeNumbers)
gv_corpus = tm_map(gv_corpus, removePunctuation)
gv_corpus = tm_map(gv_corpus, removeWords, c("the", "and", stopwords("english")))
gv_corpus =  tm_map(gv_corpus, stripWhitespace)
inspect(gv_corpus[1])
review_gv <- DocumentTermMatrix(gv_corpus)
review_gv
inspect(review_gv[500:505,500:505])
review_gv = removeSparseTerms(review_gv, 0.99)
review_gv
inspect(review_gv[1,1:20])
findFreqTerms(review_gv,1000)
freq = data.frame(sort(colSums(as.matrix(review_gv)), decreasing=TRUE))
wordcloud(rownames(freq), freq[,1], max.words=50, colors=brewer.pal(1, "Dark2"))
review_gv_tfidf <- DocumentTermMatrix(review_gv,  control = list(weighting = weightTfIdf))
removeNumPunct <- function(x) gsub("[^[:alpha:][:space:]]*","",x)
gvCorpus <- tm_map(review_gv, content_transformer(removeNumPunct), lazy = TRUE)
review_gv_tfidf <- DocumentTermMatrix(gv_corpus,  control = list(weighting = weightTfIdf))
review_gv_tfidf = removeSparseTerms(review_gv_tfidf,0.95)
review_gv_tfidf
inspect(review_gv_tfidf[1,1:20])
freq = data.frame(sort(colSums(as.matrix(review_gv_tfidf)), decreasing=TRUE))
wordcloud(rownames(freq), freq[,1], max.words=100, colors=brewer.pal(1, "Dark2"))
View(responseSA)
View(responseSA)
neg_words = read.csv("neg.csv", header = F, stringsAsFactors = F)[, 1]
neg <- read.csv("~/Desktop/neg.csv")
View(neg)
pos <- read.csv("~/Desktop/pos.csv")
View(pos)
reviews$neg = sapply(review_gv, tm_term_score, neg)
neg <- read.delim("~/Desktop/neg.csv", quote="")
View(neg)
pos <- read.csv("~/Desktop/pos.csv")
View(pos)
neg <- read.csv("~/Desktop/neg.csv")
View(neg)
reviews$neg = sapply(review_gv, tm_term_score, neg)
reviews$neg = sapply(gv_corpus, tm_term_score, neg)
gv$neg = sapply(gv_corpus, tm_term_score, neg)
gv_corpus <- tm_map(gv_corpus, tolower)
gv$neg = sapply(gv_corpus, tm_term_score, neg)
gv_corpus <- tm_map(gv_corpus, PlainTextDocument)
gv$neg = sapply(gv_corpus, tm_term_score, neg)
install.packages("http://cran.r-project.org/bin/windows/contrib/3.0/tm_0.5-10.zip")
gv$neg = sapply(gv_corpus, tm_term_score, neg)
tdm <- TermDocumentMatrix(gv_corpus)
review_gv <- weightTfIdf(review_gv, normalize=TRUE)
review_gv.matrix = as.matrix(review_gv)
wordcloud(colnames(dtm.matrix), dtm.matrix[28, ], max.words = 20)
wordcloud(colnames(review_gv.matrix), review_gv.matrix[28, ], max.words = 20)
review_gv_kmeans <- DocumentTermMatrix(gv_corpus)
inspect(gv_corpus[1])
savehistory("~/pt2.r")
gv_corpus = Corpus(VectorSource(gv$text))
gv_corpus = tm_map(review_corpus, content_transformer(tolower))
gv_corpus = tm_map(gv_corpus, content_transformer(tolower))
gv_corpus =tm_map(gv_corpus, removeNumbers)
gv_corpus = tm_map(gv_corpus, removePunctuation)
gv_corpus = tm_map(gv_corpus, removeWords, c("the", "and", stopwords("english")))
gv_corpus =  tm_map(gv_corpus, stripWhitespace)
review_gv <- DocumentTermMatrix(gv_corpus)
review_gv = removeSparseTerms(review_gv, 0.99)
findFreqTerms(review_gv,1000)
freq = data.frame(sort(colSums(as.matrix(review_gv)), decreasing=TRUE))
wordcloud(rownames(freq), freq[,1], max.words=50, colors=brewer.pal(1, "Dark2"))
dtm <- weightTfIdf(review_gv, normalize=TRUE)
dtm.matrix = as.matrix(dtm)
wordcloud(colnames(dtm.matrix), dtm.matrix[28, ], max.words = 20)
inspect(dtm)
write.csv((as.matrix(dtm)),"test.csv")
head(sort(as.matrix(dtm)[1,], decreasing = TRUE), n=15)
wordcloud(colnames(dtm.matrix), dtm.matrix[3, ], max.words = 200)
m <- as.matrix(dtm)
dtm.matrix = as.matrix(dtm)
write.csv((as.matrix(dtm)), "test.csv")
wordcloud(colnames(dtm.matrix), dtm.matrix[3, ], max.words = 200)
m  <- as.matrix(dtm)
distMatrix <- dist(m, method="euclidean")
print(distMatrix)
groups <- hclust(distmatrix, method="ward.D")
distMatrix <- dist(m, method="euclidean")
groups <- hclust(distmatrix, method="ward.D")
groups <- hclust(distMatrix, method="ward.D")
plot(groups, cex=0.9, hang=-1)
rect.hclust(groups, k=5)
kfit <- kmeans(distMatrix, 2, nstart=200)
library(cluster)
clusplot(as.matrix(d), kfit$cluster, color=T, shade=T, labels=2, lines=0)
clusplot(as.matrix(distMatrix), kfit$cluster, color=T, shade=T, labels=2, lines=0)
gv_sample <- sample(gv,5000,replace=FALSE)
gv_sample <- sample(gv,replace=FALSE)
View(gv_sample)
gv_sample <- sample(gv)
View(gv_sample)
gv_sample <- createDataPartition(dtm,p=0.7,list=FALSE)
library(caret)
gv_sample <- createDataPartition(dtm,p=0.7,list=FALSE)
require(devtools)
gv_dataframe <- as.data.frame(dtm)
gv_sample <- createDataPartition(review_gv,p=0.7,list=FALSE)
gv_dataframe <- as.data.frame(review_gv)
gv_dataframe <- as.data.frame(gv_corpus)
gv_dataframe <- as.data.frame(gv)
gv_sample <- createDataPartition(gv_dataframe,p=0.7,list=FALSE)
gv_sample <- sample(dtm,1)
matrix_sp <- as.compressed.matrix(dtm)
library(maxent)
matrix_sp <- as.compressed.matrix(dtm)
maxent(feature_matrix, code_vector, l1_regularizer = 0, l2_regularizer = 0, use_sgd = TRUE, set_heldout = 0, verbose=FALSE)
library(dfm)
install.packages("dfm")
library(dfm)
install.packages("quanteda")
library(quanteda)
maxent(feature_matrix, code_vector, l1_regularizer = 0, l2_regularizer = 0, use_sgd = TRUE, set_heldout = 0, verbose=FALSE)
max_model <- maxent(matrix_sp[,1:2000],gv$text[1:2000], use_sgd = TRUE, set_heldout = 200)
review_gv
tdm <- TermDocumentMatrix(gv_corpus)
dtms <- removeSparseTerms(review_gv,0.2)
dtms
freq <- colSums(as.matrix(review_gv))
head(table(freq),20)
head(gv_corpus)
tdm
freq <- colSums(as.matrix(gv_corpus))
freq <- colSums(as.matrix(review_gv))
length(freq)
ord <- order(freq)
m <- as.matrix(review_gv)
dim(m)
write.csv(m, file="DocumentsTermMatrix.csv")
freq <- colSums(as.matrix(dtms))
freq
freq <- colSums(as.matrix(dtm))
freq
freq <- sort(colSums(as.matrix(review_gv)), decreasing = TRUE)
head(freq,14)
findFreqTerms(review_gv, lowfreq = 50)
f <- content_transformer(function(x, pattern) gsub(pattern, "", x))
code <- tm_map(gv_corpus, f, "[!\"#$%&'*+,./)(:;<=>?@\][\\^`{|}~]„")
wf <- data.frame(word-names(freq),freq-freq)
wf <- data.frame(word=names(freq),freq-freq)
head(wf)
library(ggplot2)
p <- ggplot(subset(wf, freq>50), aes(x = reorder(word, -freq), y = freq)) +
geom_bar(stat = "identity") +
theme(axis.text.x=element_text(angle=45, hjust=1))
p
findAssocs(review_gv, c("marchforourlives", "gunviolence"),corlimit=0.85)
set.seed(142)
wordcloud(names(freq),freq,min.freq=5)
wordcloud(names(freq),freq,min.freq=100)
dtmss <- removeSparseTerms(review_gv,0.15)
dtmss
dtmss <- removeSparseTerms(review_gv,0.1)
dtmss
dtmss <- removeSparseTerms(review_gv,0.05)
dtmss <- removeSparseTerms(review_gv,0.95)
dtmss
d <- dist(t(dtmss), method="euclidian")
fit <- hclust(d=d,method="complete")
fit
plot(fit, hang=-1)
dtmss <- removeSparseTerms(review_gv,0.75)
dtmss
d <- dist(t(dtmss), method="euclidian")
fit <- hclust(d=d,method="complete")
dtmss <- removeSparseTerms(review_gv,0.40)
dtmss
dtmss <- removeSparseTerms(review_gv,0.50)
dtmss
dtmss <- removeSparseTerms(review_gv,0.60)
dtmss
dtmss <- removeSparseTerms(review_gv,0.75)
dtmss
dtmss <- removeSparseTerms(review_gv,0.70)
dtmss
dtmss <- removeSparseTerms(review_gv,0.72)
dtmss
d <- dist(t(dtmss), method="euclidian")
fit <- hclust(d=d,method="complete")
d
library(fpc)
d <- dist(t(dtmss), method="euclidian")
kfit <- kmeans(d,2)
kfit <- kmeans(d,2, replace=TRUE)
dtmss <- removeSparseTerms(gv_corpus,0.72)
gv_dtm <- DocumentTermMatrix(gv_corpus)
dtmss_gv <- removeSparseTerms(gv_dtm, 0.15)
dtmss_gv
dtmss_gv <- removeSparseTerms(gv_dtm, 0.70)
dtmss_gv
dtmss_gv <- removeSparseTerms(gv_dtm, 0.75)
dtmss_gv
dtmss_gv <- removeSparseTerms(gv_dtm, 0.95)
dtmss_gv
d <- dist(t(dtmss_gv), method="euclidian")
fit <- hclust(d=d,method="complete")
fit
plot(fit, hang=-1)
gv_corpus_clean <- tm_map(corpus, removeWords, c("„"))
gv_corpus_clean <- tm_map(gv_corpus, removeWords, c("„"))
gv_dtm <- DocumentTermMatrix(gv_corpus)
dtmss_gv <- removeSparseTerms(gv_dtm, 0.15)
dtmss_gv
dtmss_gv <- removeSparseTerms(gv_dtm, 0.75)
dtmss_gv
dtmss_gv <- removeSparseTerms(gv_dtm, 0.99)
dtmss_gv
dtmss_gv <- removeSparseTerms(gv_dtm, 0.80)
dtmss_gv
dtmss_gv <- removeSparseTerms(gv_dtm, 0.88)
dtmss_gv
dtmss_gv <- removeSparseTerms(gv_dtm, 0.90)
dtmss_gv
dtmss_gv <- removeSparseTerms(gv_dtm, 0.97)
dtmss_gv
d <- dist(t(dtmss), method="euclidian")
fit <- hclust(d=d, method="complete")
dtmss_gv <- removeSparseTerms(gv_dtm, 0.90)
dtmss_gv
d <- dist(t(dtmss), method="euclidian")
fit <- hclust(d=d, method="complete")
dtmss_gv <- removeSparseTerms(gv_dtm, 0.75)
d <- dist(t(dtmss), method="euclidian")
fit <- hclust(d=d, method="complete")
d <- dist(t(dtmss_gv), method="euclidian")
fit <- hclust(d=d, method="complete")
dtmss_gv
dtmss_gv <- removeSparseTerms(gv_dtm, 0.98)
dtmss_gv
d <- dist(t(dtmss_gv), method="euclidian")
fit <- hclust(d=d, method="complete")
fit
plot(fit, hang=-1)
plot.new()
plot(fit, hang = -1)
groups <- cutree(fit,k=6)
rect.hclust(fit, k=6, border="red")
kfit <- kmeans(d,2)
clusplot(as.matrix(d), kfit$cluster, color=T, shade=T, labels=2, lines=0)
savehistory("~/GV_Week7.r")
